{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "229977e7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb84b0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r\"C:\\Users\\Nasra\\OneDrive\\سطح المكتب\\project AI\\Ants-beesClassifier\\Data\\raw\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f9615b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In practice, very few people train an entire Convolutional Network from scratch (with random initialization), because it is relatively rare to have a dataset of sufficient size. Instead, it is common to pretrain a ConvNet on a very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000 categories), and then use the ConvNet either as an initialization or a fixed feature extractor for the task of interest.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c26606b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x14dc5e009b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "cudnn.benchmark = True\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bb4c26",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "We will use torchvision and torch.utils.data packages for loading the\n",
    "data.\n",
    "\n",
    "The problem we're going to solve today is to train a model to classify\n",
    "**ants** and **bees**. We have about 120 training images each for ants and bees.\n",
    "There are 75 validation images for each class. Usually, this is a very\n",
    "small dataset to generalize upon, if trained from scratch. Since we\n",
    "are using transfer learning, we should be able to generalize reasonably\n",
    "well.\n",
    "\n",
    "This dataset is a very small subset of imagenet.\n",
    "\n",
    ".. Note ::\n",
    "   Download the data from\n",
    "   [here](https://download.pytorch.org/tutorial/hymenoptera_data.zip)\n",
    "   and extract it to the current directory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72a2ba59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac58632",
   "metadata": {},
   "source": [
    " ImageFolder is a generic data loader where the images are arranged in this way:\n",
    "\n",
    "root/dog/xxx.png\n",
    "\n",
    "root/dog/xxy.png\n",
    "\n",
    "root/dog/xxz.png\n",
    "\n",
    "root/cat/123.png\n",
    "\n",
    "root/cat/nsdf3.png\n",
    "\n",
    "root/cat/asd932_.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e054bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de589e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7d0cf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d58df37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 243, 'val': 153}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbab341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9120b811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sizes[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836a94e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "659d6931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ants', 'bees']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = image_datasets['train'].classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189ffd20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e591ed4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4e644b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alexnet',\n",
       " 'convnext_base',\n",
       " 'convnext_large',\n",
       " 'convnext_small',\n",
       " 'convnext_tiny',\n",
       " 'deeplabv3_mobilenet_v3_large',\n",
       " 'deeplabv3_resnet101',\n",
       " 'deeplabv3_resnet50',\n",
       " 'densenet121',\n",
       " 'densenet161',\n",
       " 'densenet169',\n",
       " 'densenet201',\n",
       " 'efficientnet_b0',\n",
       " 'efficientnet_b1',\n",
       " 'efficientnet_b2',\n",
       " 'efficientnet_b3',\n",
       " 'efficientnet_b4',\n",
       " 'efficientnet_b5',\n",
       " 'efficientnet_b6',\n",
       " 'efficientnet_b7',\n",
       " 'efficientnet_v2_l',\n",
       " 'efficientnet_v2_m',\n",
       " 'efficientnet_v2_s',\n",
       " 'fasterrcnn_mobilenet_v3_large_320_fpn',\n",
       " 'fasterrcnn_mobilenet_v3_large_fpn',\n",
       " 'fasterrcnn_resnet50_fpn',\n",
       " 'fasterrcnn_resnet50_fpn_v2',\n",
       " 'fcn_resnet101',\n",
       " 'fcn_resnet50',\n",
       " 'fcos_resnet50_fpn',\n",
       " 'googlenet',\n",
       " 'inception_v3',\n",
       " 'keypointrcnn_resnet50_fpn',\n",
       " 'lraspp_mobilenet_v3_large',\n",
       " 'maskrcnn_resnet50_fpn',\n",
       " 'maskrcnn_resnet50_fpn_v2',\n",
       " 'maxvit_t',\n",
       " 'mc3_18',\n",
       " 'mnasnet0_5',\n",
       " 'mnasnet0_75',\n",
       " 'mnasnet1_0',\n",
       " 'mnasnet1_3',\n",
       " 'mobilenet_v2',\n",
       " 'mobilenet_v3_large',\n",
       " 'mobilenet_v3_small',\n",
       " 'mvit_v1_b',\n",
       " 'mvit_v2_s',\n",
       " 'quantized_googlenet',\n",
       " 'quantized_inception_v3',\n",
       " 'quantized_mobilenet_v2',\n",
       " 'quantized_mobilenet_v3_large',\n",
       " 'quantized_resnet18',\n",
       " 'quantized_resnet50',\n",
       " 'quantized_resnext101_32x8d',\n",
       " 'quantized_resnext101_64x4d',\n",
       " 'quantized_shufflenet_v2_x0_5',\n",
       " 'quantized_shufflenet_v2_x1_0',\n",
       " 'quantized_shufflenet_v2_x1_5',\n",
       " 'quantized_shufflenet_v2_x2_0',\n",
       " 'r2plus1d_18',\n",
       " 'r3d_18',\n",
       " 'raft_large',\n",
       " 'raft_small',\n",
       " 'regnet_x_16gf',\n",
       " 'regnet_x_1_6gf',\n",
       " 'regnet_x_32gf',\n",
       " 'regnet_x_3_2gf',\n",
       " 'regnet_x_400mf',\n",
       " 'regnet_x_800mf',\n",
       " 'regnet_x_8gf',\n",
       " 'regnet_y_128gf',\n",
       " 'regnet_y_16gf',\n",
       " 'regnet_y_1_6gf',\n",
       " 'regnet_y_32gf',\n",
       " 'regnet_y_3_2gf',\n",
       " 'regnet_y_400mf',\n",
       " 'regnet_y_800mf',\n",
       " 'regnet_y_8gf',\n",
       " 'resnet101',\n",
       " 'resnet152',\n",
       " 'resnet18',\n",
       " 'resnet34',\n",
       " 'resnet50',\n",
       " 'resnext101_32x8d',\n",
       " 'resnext101_64x4d',\n",
       " 'resnext50_32x4d',\n",
       " 'retinanet_resnet50_fpn',\n",
       " 'retinanet_resnet50_fpn_v2',\n",
       " 's3d',\n",
       " 'shufflenet_v2_x0_5',\n",
       " 'shufflenet_v2_x1_0',\n",
       " 'shufflenet_v2_x1_5',\n",
       " 'shufflenet_v2_x2_0',\n",
       " 'squeezenet1_0',\n",
       " 'squeezenet1_1',\n",
       " 'ssd300_vgg16',\n",
       " 'ssdlite320_mobilenet_v3_large',\n",
       " 'swin3d_b',\n",
       " 'swin3d_s',\n",
       " 'swin3d_t',\n",
       " 'swin_b',\n",
       " 'swin_s',\n",
       " 'swin_t',\n",
       " 'swin_v2_b',\n",
       " 'swin_v2_s',\n",
       " 'swin_v2_t',\n",
       " 'vgg11',\n",
       " 'vgg11_bn',\n",
       " 'vgg13',\n",
       " 'vgg13_bn',\n",
       " 'vgg16',\n",
       " 'vgg16_bn',\n",
       " 'vgg19',\n",
       " 'vgg19_bn',\n",
       " 'vit_b_16',\n",
       " 'vit_b_32',\n",
       " 'vit_h_14',\n",
       " 'vit_l_16',\n",
       " 'vit_l_32',\n",
       " 'wide_resnet101_2',\n",
       " 'wide_resnet50_2']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b88904e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15c52bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390e5862",
   "metadata": {},
   "source": [
    "## Finetuning the ConvNet\n",
    "\n",
    "Load a pretrained model and reset final fully connected layer.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7463894f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alexnet',\n",
       " 'convnext_base',\n",
       " 'convnext_large',\n",
       " 'convnext_small',\n",
       " 'convnext_tiny',\n",
       " 'deeplabv3_mobilenet_v3_large',\n",
       " 'deeplabv3_resnet101',\n",
       " 'deeplabv3_resnet50',\n",
       " 'densenet121',\n",
       " 'densenet161',\n",
       " 'densenet169',\n",
       " 'densenet201',\n",
       " 'efficientnet_b0',\n",
       " 'efficientnet_b1',\n",
       " 'efficientnet_b2',\n",
       " 'efficientnet_b3',\n",
       " 'efficientnet_b4',\n",
       " 'efficientnet_b5',\n",
       " 'efficientnet_b6',\n",
       " 'efficientnet_b7',\n",
       " 'efficientnet_v2_l',\n",
       " 'efficientnet_v2_m',\n",
       " 'efficientnet_v2_s',\n",
       " 'fasterrcnn_mobilenet_v3_large_320_fpn',\n",
       " 'fasterrcnn_mobilenet_v3_large_fpn',\n",
       " 'fasterrcnn_resnet50_fpn',\n",
       " 'fasterrcnn_resnet50_fpn_v2',\n",
       " 'fcn_resnet101',\n",
       " 'fcn_resnet50',\n",
       " 'fcos_resnet50_fpn',\n",
       " 'googlenet',\n",
       " 'inception_v3',\n",
       " 'keypointrcnn_resnet50_fpn',\n",
       " 'lraspp_mobilenet_v3_large',\n",
       " 'maskrcnn_resnet50_fpn',\n",
       " 'maskrcnn_resnet50_fpn_v2',\n",
       " 'maxvit_t',\n",
       " 'mc3_18',\n",
       " 'mnasnet0_5',\n",
       " 'mnasnet0_75',\n",
       " 'mnasnet1_0',\n",
       " 'mnasnet1_3',\n",
       " 'mobilenet_v2',\n",
       " 'mobilenet_v3_large',\n",
       " 'mobilenet_v3_small',\n",
       " 'mvit_v1_b',\n",
       " 'mvit_v2_s',\n",
       " 'quantized_googlenet',\n",
       " 'quantized_inception_v3',\n",
       " 'quantized_mobilenet_v2',\n",
       " 'quantized_mobilenet_v3_large',\n",
       " 'quantized_resnet18',\n",
       " 'quantized_resnet50',\n",
       " 'quantized_resnext101_32x8d',\n",
       " 'quantized_resnext101_64x4d',\n",
       " 'quantized_shufflenet_v2_x0_5',\n",
       " 'quantized_shufflenet_v2_x1_0',\n",
       " 'quantized_shufflenet_v2_x1_5',\n",
       " 'quantized_shufflenet_v2_x2_0',\n",
       " 'r2plus1d_18',\n",
       " 'r3d_18',\n",
       " 'raft_large',\n",
       " 'raft_small',\n",
       " 'regnet_x_16gf',\n",
       " 'regnet_x_1_6gf',\n",
       " 'regnet_x_32gf',\n",
       " 'regnet_x_3_2gf',\n",
       " 'regnet_x_400mf',\n",
       " 'regnet_x_800mf',\n",
       " 'regnet_x_8gf',\n",
       " 'regnet_y_128gf',\n",
       " 'regnet_y_16gf',\n",
       " 'regnet_y_1_6gf',\n",
       " 'regnet_y_32gf',\n",
       " 'regnet_y_3_2gf',\n",
       " 'regnet_y_400mf',\n",
       " 'regnet_y_800mf',\n",
       " 'regnet_y_8gf',\n",
       " 'resnet101',\n",
       " 'resnet152',\n",
       " 'resnet18',\n",
       " 'resnet34',\n",
       " 'resnet50',\n",
       " 'resnext101_32x8d',\n",
       " 'resnext101_64x4d',\n",
       " 'resnext50_32x4d',\n",
       " 'retinanet_resnet50_fpn',\n",
       " 'retinanet_resnet50_fpn_v2',\n",
       " 's3d',\n",
       " 'shufflenet_v2_x0_5',\n",
       " 'shufflenet_v2_x1_0',\n",
       " 'shufflenet_v2_x1_5',\n",
       " 'shufflenet_v2_x2_0',\n",
       " 'squeezenet1_0',\n",
       " 'squeezenet1_1',\n",
       " 'ssd300_vgg16',\n",
       " 'ssdlite320_mobilenet_v3_large',\n",
       " 'swin3d_b',\n",
       " 'swin3d_s',\n",
       " 'swin3d_t',\n",
       " 'swin_b',\n",
       " 'swin_s',\n",
       " 'swin_t',\n",
       " 'swin_v2_b',\n",
       " 'swin_v2_s',\n",
       " 'swin_v2_t',\n",
       " 'vgg11',\n",
       " 'vgg11_bn',\n",
       " 'vgg13',\n",
       " 'vgg13_bn',\n",
       " 'vgg16',\n",
       " 'vgg16_bn',\n",
       " 'vgg19',\n",
       " 'vgg19_bn',\n",
       " 'vit_b_16',\n",
       " 'vit_b_32',\n",
       " 'vit_h_14',\n",
       " 'vit_l_16',\n",
       " 'vit_l_32',\n",
       " 'wide_resnet101_2',\n",
       " 'wide_resnet50_2']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5d3ca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = models.resnet18(weights='IMAGENET1K_V1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4556d28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.modules of ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "505d6e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_ftrs = model_ft.fc.in_features\n",
    "num_ftrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba3791fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here the size of each output sample is set to 2.\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "# Alternatively, it can be generalized to ``nn.Linear(num_ftrs, len(class_names))\n",
    "model_ft.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "\n",
    "len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50e27cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = model_ft.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f38d2d8",
   "metadata": {},
   "source": [
    "setting up the loss function, optimizer, and learning rate scheduler for a PyTorch model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e89a80a",
   "metadata": {},
   "source": [
    "This line is defining the loss function for the model. CrossEntropyLoss is commonly used for classification tasks. It combines LogSoftMax and Negative Log Likelihood loss in one single class. It's useful when training a classification problem with multiple classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17f6212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9407d4fa",
   "metadata": {},
   "source": [
    "This line is defining the optimizer for the model. Stochastic Gradient Descent (SGD) is a popular optimization algorithm in deep learning. It updates the model parameters iteratively in the direction that minimally reduces the loss function. The learning rate (lr) is set to 0.001, and momentum is set to 0.9. Momentum is a term added to the weight update that helps accelerate SGD in the relevant direction and dampens oscillations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59be5e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cdb6fb",
   "metadata": {},
   "source": [
    "This line is defining the learning rate scheduler for the model. A learning rate scheduler adjusts the learning rate according to a schedule. In this case, a StepLR scheduler is used, which decays the learning rate by a factor of gamma every step_size epochs. Here, step_size is set to 7, and gamma is set to 0.1. This means that the learning rate will be multiplied by 0.1 every 7 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c8b3eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c24d96",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Now, let's write a general function to train a model. Here, we will\n",
    "illustrate:\n",
    "\n",
    "-  Scheduling the learning rate\n",
    "-  Saving the best model\n",
    "\n",
    "In the following, parameter ``scheduler`` is an LR scheduler object from\n",
    "``torch.optim.lr_scheduler``.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1443d0",
   "metadata": {},
   "source": [
    "This code block is creating a temporary directory and saving the state of a PyTorch model to a file within that directory. It also initializes a variable to keep track of the best accuracy achieved during training.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
    "\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "        best_acc = 0.0\n",
    "\n",
    "\n",
    "---\n",
    "with **TemporaryDirectory()** as tempdir: This line creates a temporary directory. The with statement is used here to ensure that the directory is properly cleaned up afterwards.\n",
    "\n",
    " The **TemporaryDirectory()** function from the tempfile module generates a temporary directory in a safe way, and it's automatically deleted when it's no longer needed 1.\n",
    "\n",
    "**best_model_params_path = os.path.join(tempdir, 'best_model_params.pt'):**\n",
    "This line joins the path of the temporary directory with the filename 'best_model_params.pt' to create a full path for the file where the model's parameters will be saved.\n",
    "\n",
    "**torch.save(model.state_dict(), best_model_params_path):** This line saves the state of the model to a file. **The state_dict()** function returns a dictionary containing a whole state of the model. This includes the model parameters (weights and biases), the optimizer state, the epoch number, etc.\n",
    "\n",
    " The **torch.save()** function is then used to save this state to a file 3.\n",
    "\n",
    "**best_acc = 0.0:** This line initializes a variable to keep track of the best accuracy achieved during training.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a40bdab",
   "metadata": {},
   "source": [
    "The code you've provided is performing a forward pass through a model, computing the loss, and updating the model parameters in the case of training. Here's a detailed explanation:\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**with torch.set_grad_enabled(phase == 'train'):**:\n",
    " This line sets the computation graph to build during the forward pass. If phase is 'train', the computation graph will be built, allowing us to perform backpropagation during the backward pass.\n",
    "\n",
    "  **If phase is not 'train' (i.e., it's 'val' or 'test')**, no computation graph will be built, which is useful when we want to evaluate the model without tracking the intermediate computations 1.\n",
    "\n",
    "**outputs = model(inputs)**: This line performs a forward pass through the model with the given inputs. The model's parameters are used to compute the output.\n",
    "\n",
    "**_, preds = torch.max(outputs, 1):** This line finds the maximum value along dimension 1 of the outputs tensor (which corresponds to the highest predicted class probabilities) and returns the indices of the maximum values. The underscore _ is a convention in Python to indicate that we're ignoring the first returned value (the actual maximum values), and we're only interested in the second returned value (the indices of the maximums).\n",
    "\n",
    "**loss = criterion(outputs, labels):** This line computes the loss between the model's outputs and the true labels. The criterion (loss function) measures how well the model's predictions match the true labels.\n",
    "\n",
    "if phase == 'train':: This line checks if the current phase is training. If it is, the model parameters are updated based on the computed loss.\n",
    "\n",
    "**loss.backward():** This line performs backpropagation, which computes the gradients of the loss with respect to the model parameters.\n",
    "\n",
    "**optimizer.step():** This line updates the model parameters using the computed gradients. The optimizer determines how the model parameters should be updated in order to minimize the loss\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07662bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    # Create a temporary directory to save training checkpoints\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
    "\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "        best_acc = 0.0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "            print('-' * 10)\n",
    "\n",
    "            # Each epoch has a training and validation phase\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    model.train()  # Set model to training mode\n",
    "                else:\n",
    "                    model.eval()   # Set model to evaluate mode\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                # Iterate over data.\n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "                # deep copy the model\n",
    "                if phase == 'val' and epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    #save the best model\n",
    "                    torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "            print()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "        # load best model weights\n",
    "        model.load_state_dict(torch.load(best_model_params_path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95d7788",
   "metadata": {},
   "source": [
    "### Train and evaluate\n",
    "\n",
    "It should take around 15-25 min on CPU. On GPU though, it takes less than a\n",
    "minute.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd6dd920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 0.3572 Acc: 0.8354\n",
      "val Loss: 0.3671 Acc: 0.8889\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 0.4253 Acc: 0.8148\n",
      "val Loss: 0.8751 Acc: 0.7451\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 0.3870 Acc: 0.8683\n",
      "val Loss: 0.3837 Acc: 0.8497\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 0.5249 Acc: 0.7901\n",
      "val Loss: 0.2734 Acc: 0.8758\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 0.3747 Acc: 0.8354\n",
      "val Loss: 0.3544 Acc: 0.8758\n",
      "\n",
      "Training complete in 2m 17s\n",
      "Best val Acc: 0.888889\n"
     ]
    }
   ],
   "source": [
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11ce0e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opposed to before.\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3ebbc1",
   "metadata": {},
   "source": [
    "### Train and evaluate\n",
    "\n",
    "On CPU this will take about half the time compared to previous scenario.\n",
    "This is expected as gradients don't need to be computed for most of the\n",
    "network. However, forward does need to be computed.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7512af1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "train Loss: 0.6665 Acc: 0.6461\n",
      "val Loss: 0.2532 Acc: 0.9216\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n",
      "train Loss: 0.4197 Acc: 0.8230\n",
      "val Loss: 0.2093 Acc: 0.9346\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n",
      "train Loss: 0.5221 Acc: 0.7572\n",
      "val Loss: 0.2172 Acc: 0.9346\n",
      "\n",
      "Epoch 3/9\n",
      "----------\n",
      "train Loss: 0.5682 Acc: 0.7531\n",
      "val Loss: 0.4860 Acc: 0.8039\n",
      "\n",
      "Epoch 4/9\n",
      "----------\n",
      "train Loss: 0.3473 Acc: 0.8560\n",
      "val Loss: 0.3170 Acc: 0.8627\n",
      "\n",
      "Epoch 5/9\n",
      "----------\n",
      "train Loss: 0.6142 Acc: 0.7613\n",
      "val Loss: 0.3432 Acc: 0.8693\n",
      "\n",
      "Epoch 6/9\n",
      "----------\n",
      "train Loss: 0.4555 Acc: 0.8066\n",
      "val Loss: 0.1726 Acc: 0.9608\n",
      "\n",
      "Epoch 7/9\n",
      "----------\n",
      "train Loss: 0.3578 Acc: 0.8189\n",
      "val Loss: 0.1777 Acc: 0.9542\n",
      "\n",
      "Epoch 8/9\n",
      "----------\n",
      "train Loss: 0.3814 Acc: 0.8230\n",
      "val Loss: 0.1808 Acc: 0.9477\n",
      "\n",
      "Epoch 9/9\n",
      "----------\n",
      "train Loss: 0.3443 Acc: 0.8765\n",
      "val Loss: 0.1727 Acc: 0.9542\n",
      "\n",
      "Training complete in 2m 49s\n",
      "Best val Acc: 0.960784\n"
     ]
    }
   ],
   "source": [
    "model_conv = train_model(model_conv, criterion, optimizer_conv,\n",
    "                         exp_lr_scheduler, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebc9f09",
   "metadata": {},
   "source": [
    "## Inference on custom images\n",
    "\n",
    "Use the trained model to make predictions on custom images and visualize\n",
    "the predicted class labels along with the images.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "276f0758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model_predictions(model,img_path):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    img = Image.open(img_path)\n",
    "    img = data_transforms['val'](img)\n",
    "    img = img.unsqueeze(0)\n",
    "    img = img.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        ax = plt.subplot(2,2,1)\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'Predicted: {class_names[preds[0]]}')\n",
    "        imshow(img.cpu().data[0])\n",
    "\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7ac597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09c61035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Display image for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ad8b079c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nasra\\\\OneDrive\\\\سطح المكتب\\\\project AI\\\\Ants-beesClassifier\\\\Data\\\\raw\\\\val\\\\ants\\\\800px-Meat_eater_ant_qeen_excavating_hole.jpgg'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      5\u001b[39m img_path = \u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33mC:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mUsers\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mNasra\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mOneDrive\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mسطح المكتب\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mproject AI\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mAnts-beesClassifier\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mData\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mraw\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mants\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m800px-Meat_eater_ant_qeen_excavating_hole.jpgg\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# خيار 2: forward slashes (أسهل)\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# img_path = 'C:/Users/Nasra/OneDrive/سطح المكتب/project AI/Ants-beesClassifier/Data/valduition/bees/OIP (1).jpeg'\u001b[39;00m\n\u001b[32m      9\u001b[39m \n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# خيار 3: os.path.join (الأفضل)\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m#img_path = os.path.join('\"C:\\Users\\Nasra\\OneDrive\\سطح المكتب\\project AI\\project AI222\\valduition\\bees\\OIP.jpeg\"')\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mvisualize_model_predictions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_conv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimg_path\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m plt.ioff()\n\u001b[32m     19\u001b[39m plt.show()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mvisualize_model_predictions\u001b[39m\u001b[34m(model, img_path)\u001b[39m\n\u001b[32m      2\u001b[39m was_training = model.training\n\u001b[32m      3\u001b[39m model.eval()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m img = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m img = data_transforms[\u001b[33m'\u001b[39m\u001b[33mval\u001b[39m\u001b[33m'\u001b[39m](img)\n\u001b[32m      7\u001b[39m img = img.unsqueeze(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nasra\\OneDrive\\سطح المكتب\\project AI\\Ants-beesClassifier\\.venv\\Lib\\site-packages\\PIL\\Image.py:3513\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_path(fp):\n\u001b[32m   3512\u001b[39m     filename = os.fspath(fp)\n\u001b[32m-> \u001b[39m\u001b[32m3513\u001b[39m     fp = \u001b[43mbuiltins\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   3514\u001b[39m     exclusive_fp = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3515\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Nasra\\\\OneDrive\\\\سطح المكتب\\\\project AI\\\\Ants-beesClassifier\\\\Data\\\\raw\\\\val\\\\ants\\\\800px-Meat_eater_ant_qeen_excavating_hole.jpgg'"
     ]
    }
   ],
   "source": [
    "# الحل الصحيح - استخدم raw string للمسار\n",
    "import os\n",
    "\n",
    "# خيار 1: raw string\n",
    "img_path = r'C:\\Users\\Nasra\\OneDrive\\سطح المكتب\\project AI\\Ants-beesClassifier\\Data\\raw\\val\\ants\\800px-Meat_eater_ant_qeen_excavating_hole.jpgg'\n",
    "\n",
    "# خيار 2: forward slashes (أسهل)\n",
    "# img_path = 'C:/Users/Nasra/OneDrive/سطح المكتب/project AI/Ants-beesClassifier/Data/valduition/bees/OIP (1).jpeg'\n",
    "\n",
    "# خيار 3: os.path.join (الأفضل)\n",
    "#img_path = os.path.join('\"C:\\Users\\Nasra\\OneDrive\\سطح المكتب\\project AI\\project AI222\\valduition\\bees\\OIP.jpeg\"')\n",
    "\n",
    "visualize_model_predictions(\n",
    "    model_conv,\n",
    "    img_path=img_path\n",
    ")\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbbcbb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "441dfa69",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (2932952506.py, line 3)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mimg_path='C:\\Users\\Nasra\\OneDrive\\سطح المكتب\\project AI\\Ants-beesClassifier\\Data\\valduition\\ants\\download.jpeg'\u001b[39m\n             ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "visualize_model_predictions(\n",
    "    model_conv,\n",
    "    img_path='C:\\Users\\Nasra\\OneDrive\\سطح المكتب\\project AI\\Ants-beesClassifier\\Data\\valduition\\ants\\download.jpeg'\n",
    ")\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68484d98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf76977e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "88c4bdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_conv, \"ant_bee_model2.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "287d5352",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_conv.state_dict(), \"ant_bee_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bf16b15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c = torch.load(\"ant_bee_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "02446a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: bees\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# الحل الصحيح - استخدم raw string\n",
    "img_path = r\"C:\\Users\\Nasra\\OneDrive\\سطح المكتب\\project AI\\Ants-beesClassifier\\Data\\valduition\\bees\\OIP.jpeg\"\n",
    "img = Image.open(img_path)\n",
    "img = data_transforms['val'](img)\n",
    "img = img.unsqueeze(0)\n",
    "img = img.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model_conv(img)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    print(f'Predicted: {class_names[preds[0]]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
